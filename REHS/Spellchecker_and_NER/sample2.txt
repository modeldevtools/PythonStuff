
Fri Jul 21 17:21:53 2017: Request 72485 was acted upon.
 Transaction: Queue changed from 0-Help to 0-SDSC by clm
       Queue: 0-SDSC
     Subject: XUP: MPI Problem?
       Owner: clm
  Requestors: tarakesh@asu.edu
      Status: open
 Ticket <URL: https://tickets.xsede.org/Ticket/Display.html?id=72485 >


[Ticket created from XUP by tara]
[From: Tarakeshwar Pilarisetty]
[System: Comet]
[Category: Software]
Thanks for your help. But I am now facing another problem.

Two of my jobs submitted from /home/tara/new_siesta/fe13 and /home/tara/new_siesta/fe12 are running fine.

However, a job I submitted from the /home/tara/new_siesta/fe12_c10h2 directory is exiting out with some MPI error " Too many communicators (0/16384 free on this process; ignore_id=0)". Please see the slurm output in the directory for a full description of the error. I am encountering a similar problem with the job submitted in /home/tara/new_siesta/fe13_6c2h2 directory.

I am confused with the error, because it appears after a couple of scf steps. This tells me that there is nothing wrong with my inputs.

Any pointers on the source of the error would be helpful.

Thanks in advance,

Tarakeshwar


Fri Jul 21 13:56:45 2017: Request 66933 was acted upon.
 Transaction: Correspondence added by mahidhar
       Queue: 0-SDSC
     Subject: XUP: Slurm job!
       Owner: mahidhar
  Requestors: tarakesh@asu.edu
      Status: open
 Ticket URL: https://portal.xsede.org/group/xup/tickets/-/tickets/66933


Tarakeshwar,

I think you are missing the MODULEPATH addition. If you put that in, even the

module load netcdf/4.3.2intelmpi

will work. As I mentioned we have not changed this at all. That said, using the default netcdf is fine for most cases.

-Mahidhar
test test test
