
Sat Jun 02 21:14:32 2018: Request 89462 was acted upon.
 Transaction: Queue changed from 0-Help to 0-SDSC by carmanh
       Queue: 0-SDSC
     Subject: XUP: Job memory limit
       Owner: Nobody
  Requestors: avivsolo@gmail.com
      Status: open
 Ticket <URL: https://tickets.xsede.org/Ticket/Display.html?id=89462 >


[Ticket created from XUP by guest]
[From: Aviv Solodoch]
[System: Comet]
[Category: Batch Queues/Jobs]
Hello
I submitted a serial job (preprocessing for a later job which will be MPI parallel), which was stopped for, as I understnad, not enough memory  ("Exceeded step memory limit at some point"). The .out file ("serial...") and batch file ("Concat...") are attached. Both of these files are also found at the run directory:  /oasis/scratch/comet/avivsolo/temp_project/GB_B/Input/blk
I already increased the requested memory to 30GB. I'm not sure if that works. The job is supposed to concatenate all netcdf files in the directory to 1 file, almost 0.9 TB large, which is needed for input to a later (MPI) job. The job appears to have stopped at a point in which the output file was about 0.6TB large, i.e., 2/3 of the way through. Any advice would be appreciated.
REgards,
Aviv Solodoch 
